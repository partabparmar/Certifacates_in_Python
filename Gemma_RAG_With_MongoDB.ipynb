{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/partabparmar/Certifacates_in_Python/blob/main/Gemma_RAG_With_MongoDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baA7AWdBvil5"
      },
      "source": [
        "# Step-by-Step Tutorial to Build a RAG using Google Gemma and MongoDB Atlas\n",
        "In this step-by-step tutorial, we will use a simple Retrieval Augmented Generation (RAG) with Google Gemma to help it answer travel-related queries.\n",
        "\n",
        "Before we start, let’s look into what RAG is.\n",
        "\n",
        "**Retrieval Augmented Generation**, aka RAG, is often described as an “open-book” approach to answering domain-specific questions.\n",
        "\n",
        "Typically, Large Language Models (LLMs) are trained on vast amounts of data, but sometimes they need updates to provide accurate information for specific topics.\n",
        "\n",
        "There are two main methods:\n",
        "\n",
        "\n",
        "1. Fine-tuning the model with specific data or giving it extra context with user queries.\n",
        "\n",
        "2. Using Retrieval Augmented Generation (RAG): This is like an open-book test for the model, where it can find relevant answers. This approach is often preferred due to lower computational costs compared to fine-tuning.\n",
        "\n",
        "For more details, you can explore RAG [here](https://arxiv.org/abs/2005.11401).\n",
        "\n",
        "Now, let’s talk about enhancing Google Gemma for travel-related questions.\n",
        "\n",
        "Below is our architecture.\n",
        "\n",
        "![RAG](https://drive.google.com/uc?id=1Y6IT6wKrDwjOIX-3IZlq5LVlHRcEHebC)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oklx1_U5f1cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCGrXP8Dv-Ph"
      },
      "source": [
        "## Step 0: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOvIbN-NYaGS"
      },
      "outputs": [],
      "source": [
        "!pip install datasets pymongo sentence_transformers gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fSNpbmPph07"
      },
      "source": [
        "## Step 1: Building our Documents Database\n",
        "![Dataset loading](https://drive.google.com/uc?id=1fC7fCFRxfgzC1sfemaJEK5fGlk3yqqpY)\n",
        "\n",
        "We assembled our Documents database using information from Wikivoyage, a freely accessible online travel guide authored by volunteers.\n",
        "\n",
        ">*Note: While this initial iteration suffices for prototyping purposes, future iterations could benefit from additional data and chunking techniques for enhanced production readiness.*\n",
        "\n",
        "However, for this tutorial, we have already processed and parsed the data for you and will be directly using it from HuggingFace.\n",
        "\n",
        "We will be using the `ashmib/wikivoyage-eu-city-embeddings` dataset here.\n",
        "This dataset comprises abstracts from `Wikivoyage` for 160 European cities along with their corresponding country `names`, `coordinates`, and `populations`. The embeddings are derived from the `GTE-Large model`, incorporating data from the city, country, population, and abstract columns.\n",
        "\n",
        "We load it from HuggingFace and transform it into a `pandas` dataframe.\n",
        "\n",
        "Link to the dataset: https://huggingface.co/datasets/ashmib/wikivoyage-eu-city-embeddings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmVPX48yTBcH"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"ashmib/wikivoyage-eu-city-embeddings\", download_mode=\"force_redownload\") ## downloading it from HuggingFace\n",
        "dataset.set_format(type='pandas') ## converting it into pandas\n",
        "df = dataset[\"train\"][:]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jYZtdLrrtfT"
      },
      "source": [
        "## Step 2: Embedding Model\n",
        "![Embedding.png](https://drive.google.com/uc?id=1yMntmFJdugi1ib-JNOoqlo_WbPgWvDy3)\n",
        "\n",
        "The next step is to generate the embeddings that will be used to embed both the documents and the queries.\n",
        "\n",
        "\n",
        "> Embeddings are vector representations of textual data, aiding in semantic understanding and similarity comparisons.\n",
        "\n",
        "\n",
        "The embeddings enhance the efficiency and accuracy of information retrieval systems by encoding semantic relationships between documents and queries.\n",
        "\n",
        "In this case, we utilize the `gte-large` embedding model from the `SentenceTransformer` library to generate embeddings for documents and queries.\n",
        "\n",
        "> *Note: In practice, you should also compute the embeddings for your documents. However, for this tutorial, we have you covered. We will only use the embeddings function to compute the embeddings for the user query.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhf58jUQKHA4"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def get_embedding(text: str) -> list[float]:\n",
        "\n",
        "    embedding_model = SentenceTransformer(\"thenlper/gte-large\")\n",
        "    if not text.strip():\n",
        "        print(\"Attempted to get embedding for empty text.\")\n",
        "        return []\n",
        "\n",
        "    embedding = embedding_model.encode(text)\n",
        "\n",
        "    return embedding.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O-xy4_Abbms"
      },
      "source": [
        "## Step 3: Vector Database Setup & Data Ingestion\n",
        "\n",
        "Here, we use MongoDB as the operational and vector database.\n",
        "\n",
        "![Vector DB setup](https://drive.google.com/uc?id=1Z82OKKV72GvEDuhjRnFkMkGiuZ73BD3k)\n",
        "\n",
        "1. We create an account on [MongoDB Atlas](https://www.mongodb.com/cloud/atlas/register) and note the username, password and connection string (Mongo URI).\n",
        "2. Add the username and password as secrets to the Colab or environment variables\n",
        "3. We utilize the `pymongo` library to establish database connectivity\n",
        "4. Then we migrate our tabular data into the collections we just created using\n",
        "\n",
        "    ```\n",
        "documents = df.to_dict('records')\n",
        "collection.insert_many(documents)\n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8c6ojQab-Ii"
      },
      "outputs": [],
      "source": [
        "import pymongo\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "def get_mongo_url():\n",
        "    username = userdata.get(\"MONGO_USERNAME\")\n",
        "    password = userdata.get(\"MONGO_PW\")\n",
        "    # uri_string = \"@cluster0.62unmco.mongodb.net/\"\n",
        "    uri_string = userdata.get(\"MONGO_CONN_STR\")\n",
        "    mongo_url = f\"mongodb+srv://{username}:{password}{uri_string}\"\n",
        "    return mongo_url\n",
        "\n",
        "\n",
        "def get_mongo_client(mongo_url):\n",
        "    \"\"\"Establish connection to the MongoDB.\"\"\"\n",
        "    if not mongo_url:\n",
        "        print(\"MONGO_URI not set in environment variables\")\n",
        "    try:\n",
        "        client = pymongo.MongoClient(mongo_url)\n",
        "        print(\"Connection to MongoDB successful\")\n",
        "        return client\n",
        "    except pymongo.errors.ConnectionFailure as e:\n",
        "        print(f\"Connection failed: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaMhS0WgfACe"
      },
      "outputs": [],
      "source": [
        "## Data ingestion in Mongodb\n",
        "\n",
        "mongo_url = get_mongo_url()\n",
        "if not mongo_url:\n",
        "    print(\"MONGO_creds not set in environment variables\")\n",
        "\n",
        "# establishes database connection\n",
        "mongo_client = get_mongo_client(mongo_url)\n",
        "# creates database\n",
        "db = mongo_client[\"wikivoyage_cities\"]\n",
        "# creates collection\n",
        "collection = db[\"wikivoyage_collection_2\"]\n",
        "\n",
        "# Delete any existing records in the collection\n",
        "collection.delete_many({})\n",
        "\n",
        "# data ingestion into mongoDB\n",
        "documents = df.to_dict('records')\n",
        "collection.insert_many(documents)\n",
        "print(\"Data ingestion into MongoDB completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zU5SyLRdCxc"
      },
      "source": [
        "## Step 4: Retrieval\n",
        "![Retrieval](https://drive.google.com/uc?id=1Qnk-v5ZGFo1DN3Ye6iylcL4hOL9ATHjR)\n",
        "\n",
        "In this step, we retrieve the most similar documents to the query from the vector database.\n",
        "\n",
        "In the `query_results` function\n",
        "1. `[Line 2-3]`: We connect to the Mongo Client\n",
        "2. `[Line 5]`: We retrieve the embedding for the input query using the `get_embedding` function\n",
        "3. `[Line 6-16]`: The MongoDB aggregation framework is then used to perform a `vector search` on the collection named `“EU_cities_collection.”` The vector search is performed using a specified index (`“vector_index”`) and the path to the embedding field, using the query_embedding as the vector for the search. It specifies that up to 150 candidates should be retrieved but limits the final results to the top 5 candidates.\n",
        "4. In the `get_search_result` function:\n",
        "\n",
        "    We begin by calling the `query_results` function with the provided `query` and `mongo_url`. This returns the `5 top most similar documents` from our database, i.e., the context that is then augmented with the prompt later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChgoqobnEhjg"
      },
      "outputs": [],
      "source": [
        "def query_results(query, mongo_url):\n",
        "    mongo_client = get_mongo_client(mongo_url)\n",
        "    db = mongo_client[\"wikivoyage_cities\"]\n",
        "\n",
        "    query_embedding = get_embedding(query)\n",
        "    results = db.wikivoyage_collection_2.aggregate([\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": \"vector_index\",\n",
        "                \"path\": \"embedding\",\n",
        "                \"queryVector\": query_embedding,\n",
        "                \"numCandidates\": 150,\n",
        "                \"limit\": 5\n",
        "            }\n",
        "        }\n",
        "    ])\n",
        "    return results\n",
        "\n",
        "def get_search_result(query, mongo_url):\n",
        "    get_knowledge = query_results(query, mongo_url)\n",
        "    print(get_knowledge)\n",
        "\n",
        "    search_result = \"\"\n",
        "    for result in get_knowledge:\n",
        "        search_result += f\"City: {result.get('city', 'N/A')}, Abstract: {result.get('combined', 'N/A')}\\n\"\n",
        "\n",
        "    return search_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSoZEBZ2EtUN"
      },
      "source": [
        "## Step 5: Augmentation and Generation\n",
        "![Augmentation & Generation](https://drive.google.com/uc?id=11-Hn2WbD8_KqsQpxny9ovH1IZa78r7PW)\n",
        "\n",
        "0. From the above steps, we have our context (derived from the `get_search_result` function with the user query).\n",
        "1. We combine it with the user prompt to create an augmented prompt.\n",
        "2. This augmented prompt serves as an input for our chosen LLm to generate a context specific result.\n",
        "3. In this instance, we utilize the `google/gemma-2b-it` model from the `HuggingFace InferenceClient` for `text generation`.\n",
        "    Link to the model: https://huggingface.co/google/gemma-2b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64weAGdcExB1"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "HF_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "def generate_text(query, model_name: str | None = \"google/gemma-2b-it\"):\n",
        "    if model_name is None:\n",
        "        model_name = \"google/gemma-2b-it\"\n",
        "\n",
        "    # establish mongo connection\n",
        "    mongo_url = get_mongo_url()\n",
        "\n",
        "    # get the top 5 most similar documents from the Vector data base\n",
        "    source_information = get_search_result(query, mongo_url)\n",
        "\n",
        "    # augment the query with the context\n",
        "    combined_information = (\n",
        "        f\"Query: {query}\\nContinue to answer the query by using the Search Results:\\n{source_information}.\"\n",
        "    )\n",
        "\n",
        "    # use the HF inference client to generate the text\n",
        "    client = InferenceClient(model_name, token=HF_token)\n",
        "    stream = client.text_generation(prompt=combined_information, details=True, stream=True, max_new_tokens=2048,\n",
        "                                    return_full_text=False)\n",
        "   # formatting the output\n",
        "    output = \"\"\n",
        "\n",
        "    for response in stream:\n",
        "        output += response.token.text\n",
        "\n",
        "    if \"<eos>\" in output:\n",
        "        output = output.split(\"<eos>\")[0]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXyZ-tQBJkTG"
      },
      "outputs": [],
      "source": [
        "query = \"I am planning a vacation to Spain. Can you suggest a one-week itinerary including must-visit places and local cuisines to try?\"\n",
        "generate_text(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Thk_kbLoUIw8"
      },
      "source": [
        "## Step 6: Gradio Interface\n",
        "[Gradio](https://www.gradio.app/docs) is an open-source Python package that simplifies building demos or web applications without requiring any JavaScript, CSS, or web hosting expertise.\n",
        "\n",
        "We use this to host our application and create demos.\n",
        "The refined code for this can be found [here](https://huggingface.co/spaces/ashmib/gemma-gemini-eu-travels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_7OCokfAxxp"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "examples = [[\"I'm planning a vacation to France. Can you suggest a one-week itinerary including must-visit places and \"\n",
        "             \"local cuisines to try?\", None],\n",
        "            [\"Recommend places that are similar to Istanbul in terms of architecture\", None],\n",
        "            ]\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=[\"text\",\n",
        "            gr.Dropdown(\n",
        "                [\"google/gemma-2b-it\",], label=\"Models\", info=\"Will \"\n",
        "                                                                                                             \"add \"\n",
        "                                                                                                             \"more \"\n",
        "                                                                                                             \"models \"\n",
        "                                                                                                             \"later! \"\n",
        "            ),\n",
        "            ],\n",
        "    title=\"🇪🇺 Euro City TravelBot 🇪🇺\",\n",
        "    description=\"Travel related queries for Europe.\",\n",
        "    outputs=[\"text\"],\n",
        "    examples=examples,\n",
        ")\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b5hYh-C4wWq"
      },
      "source": [
        "## Demo\n",
        "![HuggingFace Spaces Demo](https://drive.google.com/uc?id=1TXKzPJ3-KyOKIv703MFf4h3MAyYBP3RS)\n",
        "\n",
        "\n",
        "https://bit.ly/eu-city-reco-app\n",
        "\n",
        "\n",
        "## References and Further Reading\n",
        "\n",
        "* https://bit.ly/llm4rec\n",
        "\n",
        "## Workshop Feedback\n",
        "* https://bit.ly/ashmib-feedback\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}